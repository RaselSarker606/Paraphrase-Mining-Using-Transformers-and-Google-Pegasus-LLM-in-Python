{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#How to paraphrase text using transformers in Python"
      ],
      "metadata": {
        "id": "XDwCt-aouxzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes and References:\n",
        "\n",
        "PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization) is a sequence-to-sequence model developed by Google Research. In this work, the PEGASUS model has been used from the Huggingface Transformers library. For more details, refer to the original paper: PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization."
      ],
      "metadata": {
        "id": "mQKlFPuUva8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Install Libraries"
      ],
      "metadata": {
        "id": "3escd3Z-vfxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-splitter\n",
        "!pip install transformers\n",
        "!pip install SentencePiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPuZy4FUvaeN",
        "outputId": "56c5c352-52d9-463d-d109-2731e4dec17b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-splitter in /usr/local/lib/python3.11/dist-packages (1.4)\n",
            "Requirement already satisfied: regex>=2017.12.12 in /usr/local/lib/python3.11/dist-packages (from sentence-splitter) (2024.11.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Download pretrained Google Pegasus Paraphrase Model and its tokenizer"
      ],
      "metadata": {
        "id": "iCt00CH6wuKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
      ],
      "metadata": {
        "id": "nDk_E0cyvA57"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PegasusTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3mUe-F8u3ak",
        "outputId": "731fad84-37cd-452b-a59d-ecaedfae4bec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization"
      ],
      "metadata": {
        "id": "JbDaQ1g_xcwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFBfbUCBt_0u",
        "outputId": "8cd8a9be-fcf1-4937-e967-46871b97f716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Keyword arguments {'tuncation': True} not recognized.\n"
          ]
        }
      ],
      "source": [
        "text = \"Machine Learning is Fast-Growing Field in Artificial Intelligence\"\n",
        "\n",
        "batch = tokenizer([text], padding=True, tuncation=True, max_length=60, return_tensors='pt') #Pytorch\n",
        "output = model.generate(**batch, max_length=60, num_beams=5, num_return_sequences=5)\n",
        "\n",
        "results = tokenizer.batch_decode(output, skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38aMAvRH0a8Q",
        "outputId": "47621d28-ab03-46a5-b4a0-4d27d3218607"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Machine Learning is a fast-growing field.',\n",
              " 'Machine learning is growing fast.',\n",
              " 'Machine learning is a fast-growing field.',\n",
              " 'Machine Learning is growing fast.',\n",
              " 'The field of machine learning is growing fast.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predictive System (Generate Paraphrase)"
      ],
      "metadata": {
        "id": "CCLs1dQ005-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(input_text, num_return_sequences, num_beams):\n",
        "  tokenize = tokenizer([input_text], truncation=True, padding='longest', max_length=60, return_tensors='pt')\n",
        "  train = model.generate(**tokenize, max_length=60, num_beams=num_beams, num_return_sequences=num_return_sequences)\n",
        "  decode = tokenizer.batch_decode(train, skip_special_tokens=True)\n",
        "  return decode"
      ],
      "metadata": {
        "id": "w3JlBVLx02k7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_beams = 10\n",
        "num_return_sequences = 10\n",
        "text = \"If you want, I can give you a few other options\"\n",
        "get_response(text, num_return_sequences, num_beams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw8rEC7K3yMd",
        "outputId": "51dd34e9-3a7f-4c5d-92ab-2ee3146e8663"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I can give you other options if you want.',\n",
              " 'I can give you other options if you want to.',\n",
              " 'I can give you other options if you want them.',\n",
              " 'I can give you a few other options if you want.',\n",
              " 'I can give you a few other options.',\n",
              " 'I can give you a few other options if you want to.',\n",
              " 'I can give you a few other options if you want them.',\n",
              " 'I can give you more options if you want.',\n",
              " 'If you want, I can give you other options.',\n",
              " 'I can give you more options if you want to.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save model and tokenizer"
      ],
      "metadata": {
        "id": "PIigvXCj7nxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/Ai class diu Project/model300')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/Ai class diu Project/tokenizer300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "C3-Rnmyd74i4",
        "outputId": "591ce6b3-e809-44f1-fea3-8f913cb6998e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 60, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Ai class diu Project/tokenizer3/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Ai class diu Project/tokenizer3/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Ai class diu Project/tokenizer3/spiece.model',\n",
              " '/content/drive/MyDrive/Ai class diu Project/tokenizer3/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}